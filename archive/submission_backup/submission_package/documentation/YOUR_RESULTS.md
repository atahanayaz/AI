# YOUR PROJECT RESULTS - Quick Reference for Documentation

**Date:** 2026-02-08
**Album Numbers:** 103569, 103512
**Sum:** 207081

## Dataset Calculations
- **GA Dataset Number:** 1 + (207081 mod 15) = **7**
- **FL Dataset Number:** 1 + (207081 mod 29) = **22**

---

## PART 1: DECISION TREES - Heart Disease Classification

### Dataset
- **Name:** UCI Heart Disease Dataset
- **Source:** UCI Machine Learning Repository
- **Samples:** 303 patients
- **Features:** 13 clinical features (age, sex, chest pain type, blood pressure, cholesterol, etc.)
- **Target:** Binary classification (0 = No disease, 1 = Disease)

### Results
- **Training Accuracy:** 84.39%
- **Testing Accuracy:** 80.00%
- **Split:** 80% train, 20% test
- **Algorithm:** DecisionTreeClassifier (scikit-learn)
- **Best Depth:** Optimized through cross-validation

### Visualizations
- `part1-decision-trees/tree_visualization.png` (613 KB)

### Key Insights
- Model achieved good generalization (80% test accuracy)
- Tree visualization shows interpretable decision rules
- Most important features identified through feature importance analysis

---

## PART 2: NAIVE BAYES CLASSIFIER - Email Spam Detection

### Dataset (Own Data)
- **Name:** Custom Email Spam Dataset
- **Samples:** 30 emails (15 ham, 15 spam)
- **Features:** 5 binary/numerical features:
  - contains_money (binary)
  - contains_free (binary)
  - contains_click (binary)
  - word_count (numerical)
  - has_urgent (binary)
- **Target:** Binary classification (Ham vs Spam)

### Manual Calculations
- Performed on 3 test examples
- Calculated prior probabilities: P(Ham) and P(Spam)
- Calculated likelihoods: P(Feature|Class)
- Applied Bayes theorem to get posterior probabilities
- Documented in `part2-naive-bayes/NBC_manual_calculations.md`

### Python Implementation Results

**Three Models Tested:**

1. **Bernoulli Naive Bayes**
   - Training Accuracy: 95.83%
   - Testing Accuracy: 100.00%

2. **Gaussian Naive Bayes**
   - Training Accuracy: 100.00%
   - Testing Accuracy: 100.00%

3. **Multinomial Naive Bayes**
   - Training Accuracy: 95.83%
   - Testing Accuracy: 100.00%

**Cross-Validation (5-Fold):**
- Mean Accuracy: 96.67% (Â± 13.33%)
- All models achieved 100% test accuracy

### Test Examples
- **Test 1:** Email with money, free, click, urgent â†’ **SPAM** (99.87% confidence)
- **Test 2:** Normal email, no spam words â†’ **HAM** (99.69% confidence)
- **Test 3:** Email with money, click, urgent â†’ **SPAM** (99.44% confidence)

---

## PART 3: GENETIC ALGORITHMS - Knapsack Problem (Dataset #7)

### Problem Description
- **Dataset:** #7 from problem_plecakowy_zestawy - ANG.xlsx
- **Type:** 0/1 Knapsack optimization
- **Objective:** Maximize total value while staying within weight capacity

### Algorithm Parameters
- **Population Size:** 10 chromosomes
- **Generations:** 100
- **Mutation Rate:** 0.1 (10%)
- **Crossover Rate:** 0.8 (80%)
- **Selection:** Roulette wheel selection
- **Crossover:** Single-point crossover
- **Mutation:** Bit-flip mutation

### Results
- **Initial Best Fitness:** 260
- **Final Best Fitness:** 280
- **Improvement:** 20 (7.7% increase)
- **Convergence:** Generation 17
- **Success Rate:** 100% (optimal solution found)

### Evolution Statistics
- Started with random population
- Steady improvement over first 17 generations
- Solution stabilized after generation 17
- No overfitting observed

### Visualizations
- `fitness_evolution.png` - Shows fitness improvement over 100 generations
- `best_solution_visualization.png` - Visual representation of selected items
- `capacity_utilization.png` - Weight capacity usage analysis

---

## PART 4: FUZZY LOGIC - Restaurant Tip Calculator (Dataset #22)

### Problem Description
- **Application:** Automated tip calculation system
- **Dataset:** #22 (Restaurant service evaluation)
- **Type:** Mamdani fuzzy inference system

### Fuzzy System Design

**Input Variables:**
1. **Food Quality** (0-10 scale)
   - Poor: [0, 0, 4]
   - Average: [2, 5, 8]
   - Excellent: [6, 10, 10]

2. **Service Quality** (0-10 scale)
   - Poor: [0, 0, 4]
   - Average: [2, 5, 8]
   - Excellent: [6, 10, 10]

**Output Variable:**
- **Tip Percentage** (0-25%)
  - Low: [0, 0, 10]
  - Medium: [5, 15, 20]
  - High: [15, 25, 25]

**Fuzzy Rules (9 rules total):**
1. IF Food=Poor AND Service=Poor THEN Tip=Low
2. IF Food=Poor AND Service=Average THEN Tip=Low
3. IF Food=Poor AND Service=Excellent THEN Tip=Medium
4. IF Food=Average AND Service=Poor THEN Tip=Low
5. IF Food=Average AND Service=Average THEN Tip=Medium
6. IF Food=Average AND Service=Excellent THEN Tip=High
7. IF Food=Excellent AND Service=Poor THEN Tip=Medium
8. IF Food=Excellent AND Service=Average THEN Tip=High
9. IF Food=Excellent AND Service=Excellent THEN Tip=High

### Test Results

**Test Case 1: Poor Food, Poor Service**
- Food Quality: 2/10
- Service Quality: 2/10
- Recommended Tip: **6.86%**

**Test Case 2: Excellent Food, Excellent Service**
- Food Quality: 9/10
- Service Quality: 9.5/10
- Recommended Tip: **23.28%**

**Test Case 3: Average Food, Good Service**
- Food Quality: 5/10
- Service Quality: 7/10
- Recommended Tip: **20.32%**

**Test Case 4: Good Food, Average Service**
- Food Quality: 7.5/10
- Service Quality: 5/10
- Recommended Tip: **21.68%**

### Defuzzification Method
- **Method:** Centroid (center of gravity)
- **Implementation:** scikit-fuzzy library

### Visualizations
- `membership_functions.png` (230 KB) - Shows all fuzzy sets
- `output_surface.png` (803 KB) - 3D surface plot of tip vs inputs
- `contour_map.png` (408 KB) - 2D contour map of decision surface

---

## FILES TO INCLUDE IN SUBMISSION

### Code Files
1. `part1-decision-trees/DT_analysis.ipynb` (original)
2. `part1-decision-trees/DT_test_output.ipynb` (996 KB with results)
3. `part2-naive-bayes/NBC_implementation.ipynb` (original)
4. `part2-naive-bayes/NBC_test_output.ipynb` (320 KB with results)
5. `part3-genetic-algorithms/GA_implementation.ipynb` (original)
6. `part3-genetic-algorithms/GA_test_output.ipynb` (238 KB with results)
7. `part4-fuzzy-logic/FL_implementation.ipynb` (original)
8. `part4-fuzzy-logic/FL_final_output.ipynb` (1.1 MB with results)

### Data Files
1. `part1-decision-trees/data/heart.csv`
2. `part2-naive-bayes/data/email_spam.csv`
3. Dataset calculation documentation

### Visualization Files (7 total)
1. `part1-decision-trees/tree_visualization.png` (613 KB)
2. `part3-genetic-algorithms/fitness_evolution.png` (273 KB)
3. `part3-genetic-algorithms/best_solution_visualization.png` (171 KB)
4. `part3-genetic-algorithms/capacity_utilization.png` (108 KB)
5. `part4-fuzzy-logic/membership_functions.png` (230 KB)
6. `part4-fuzzy-logic/output_surface.png` (803 KB)
7. `part4-fuzzy-logic/contour_map.png` (408 KB)

### Documentation Files
1. Final report (PDF) - TO CREATE
2. Presentation slides (PPTX/PDF) - TO CREATE
3. README.md (project overview)

---

## SUMMARY STATISTICS

**Total Code Files:** 8 notebooks
**Total Data Size:** 3.6 MB
**Total Visualization Files:** 7 PNG images
**Total Implementation Time:** ~40 hours

**Achievement:**
- âœ… All 4 parts implemented and tested
- âœ… All requirements met for 5.0 grade
- âœ… Custom dataset created for Naive Bayes
- âœ… From-scratch GA implementation
- âœ… Complete FL design and implementation
- âœ… Comprehensive EDA for Decision Trees
- âœ… 100% working, tested code

---

## NEXT STEPS

1. **Fill in documentation template** using this data
2. **Create presentation slides** (15-20 slides)
3. **Practice presentation** (10-15 minutes)
4. **Final review** before submission
5. **Submit to Moodle** before 11.02.2026

---

**Good luck with your documentation!** ðŸš€
